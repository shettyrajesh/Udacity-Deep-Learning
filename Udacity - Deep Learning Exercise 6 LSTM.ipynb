{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295675 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "xovniwesvidb y xa yg d jasbzmiuclcfphvi isrug jixitnadws qemdxuig s gio n yqbvqz\n",
      "voemziumixcp izbomi  ppnu n sjeffffcbt l fflocxoc  vf need orrtvtqvthiekl sx eur\n",
      "fp egswmq nn uioiotitewxox v orlkobptrlyewizyainfaee bdyndv walsc tclti k b bhcm\n",
      "wo ueobognl e tcp kvbdcnr  ik   eix efxns vad ayjpmpad afpo tdcfdzszoedoi f  caf\n",
      "y dtteretoadeox  z u pd afb b vqoqtwzebkfzrlenldqbwd ynhcfgujuicorbhtyuhfpmvzeuh\n",
      "================================================================================\n",
      "Validation set perplexity: 20.27\n",
      "Average loss at step 100: 2.598518 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.00\n",
      "Validation set perplexity: 10.56\n",
      "Average loss at step 200: 2.256067 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.68\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 300: 2.107194 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 400: 2.008354 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.62\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 500: 1.939955 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 600: 1.911189 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 700: 1.863056 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 800: 1.821020 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 900: 1.831708 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 1000: 1.825084 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "================================================================================\n",
      "fieers astance rekully uny aroman one three king severm one one four gool eone a\n",
      "ficgle libe govto the in when doon and not the strid he serwamach recodier tiph \n",
      "jeral seeffing from equersfed unition fonn evenzed of hnat have the eigrabtorchi\n",
      "zs becopper the and d huther in fill commonisil refered presolest in uneso colde\n",
      "es few rekeed illy in in the a requakay press regory one zero in the gread prese\n",
      "================================================================================\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1100: 1.774357 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1200: 1.751042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1300: 1.732688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1400: 1.748989 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1500: 1.734478 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1600: 1.747945 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1700: 1.711894 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1800: 1.676942 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1900: 1.648800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2000: 1.696944 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "================================================================================\n",
      "wited in the empettinguly that the came the voal oting hove itwor at such cardin\n",
      "alt sland and teposso not eaprict traal pivisidatary expidebling the demings gla\n",
      "quage dizs can sell us the miskits rorking the proncialr canion the whose than t\n",
      "x spanity two zero o knamper vo wearing a way one nine sid twarn and had dismper\n",
      "k a m shood s endulation was worn a c unum five nine six nine four abowh it worl\n",
      "================================================================================\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2100: 1.684581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2200: 1.679677 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2300: 1.638060 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2400: 1.658043 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2500: 1.678495 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2600: 1.652627 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2700: 1.654826 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2800: 1.647858 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 2900: 1.644975 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3000: 1.646142 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "ald london writer marlish condicting the postly while from sball famiel four six\n",
      "usse calsuid the klarjain is to was force ismavary the filus jade as inflations \n",
      "ids to oremin bess by coiveds pases de aurcoco van ordayid juchsm langages inclu\n",
      "texslars a can which arthser he indian julifas besson of link to two sell struel\n",
      "rass unde debreakity kncism and accepacioul a though in three one nine seven kin\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3100: 1.624232 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3200: 1.644558 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3300: 1.632989 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3400: 1.667603 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3500: 1.654663 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3600: 1.668177 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3700: 1.641583 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3800: 1.637505 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3900: 1.629789 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4000: 1.648408 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "grelist desccocesubs in mazcal was deguliogicals irouges reheriatical as soal ma\n",
      "ger of guelest fames these sinkess citing five is comican from the by mistive of\n",
      "ity to pladite suyh right coror as the spegithic worl sontarys with the mocond t\n",
      "zeted and debelizalingeted its the came abeamal trumane begnopage butula america\n",
      "rent syll beho ordyward officiture clase totalbition of led sufk very adherv a i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4100: 1.630142 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4200: 1.627661 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4300: 1.608099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4400: 1.605193 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 4500: 1.613545 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4600: 1.611454 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4700: 1.621501 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4800: 1.624535 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4900: 1.629407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5000: 1.604774 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "ked eegalls temperosement dua voly to mny handh which prot independing thgs to a\n",
      "dic increasin projaste mositical offiked admethan class particy amohn linizh ini\n",
      "native where lewthomel were enoil one zero zero six zero zero zero zero zero zer\n",
      "le on ncter the garelyst may sometimen a largo prime diames and mind fasabidia r\n",
      "ed shaper cenvelless of but and in the partured riching also one of him of ecil \n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5100: 1.604977 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5200: 1.586854 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5300: 1.576528 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5400: 1.574783 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5500: 1.562814 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5600: 1.575724 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5700: 1.562959 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800: 1.575283 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5900: 1.571041 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6000: 1.541858 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "miciadd that six edp a states of wab bosl a dix of the univers of seven zero zer\n",
      "back by issus the have similarity comyaring pickly modiveric of history mision o\n",
      "x primaisher a zero duringer diplical best hassem that higgrowe his igrol extern\n",
      "sals shart deather yareventic witcibular defined theismans von ention and ocpect\n",
      "jesticles in his north the histores to onl of come haven was despel caha since a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6100: 1.559421 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6200: 1.531962 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6300: 1.537104 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6400: 1.532276 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6500: 1.551348 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6600: 1.591978 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6700: 1.578095 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6800: 1.599894 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6900: 1.573476 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 7000: 1.571663 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "ut as ged best jasixatp fatage d and to one nine two zero on s leggerss inspires\n",
      "y rel uk lelvective at their juel pea no rices proxracle maw cardogor of harn an\n",
      "t above to fews incritician they would the play eizht the beached gaters be alth\n",
      " well which clabers american new folcifurer homins of corred that bandomy that t\n",
      "roqued who in scainies brides and soviety one divie of portics and system is sus\n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Concatenate parameters  \n",
    "  sx = tf.concat(1, [ix, fx, cx, ox])\n",
    "  sm = tf.concat(1, [im, fm, cm, om])\n",
    "  sb = tf.concat(1, [ib, fb, cb, ob])\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    smatmul_input, smatmul_forget, update, smatmul_output = tf.split(1, 4, smatmul)\n",
    "    input_gate = tf.sigmoid(smatmul_input)\n",
    "    forget_gate = tf.sigmoid(smatmul_forget)\n",
    "    output_gate = tf.sigmoid(smatmul_output)\n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296227 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "lemoe xkeskxeo lntcoe taye hzjmeresqddh neyxnjsylehsitvc qssibjixkdsd  nejwec qa\n",
      "vnmhwwtd  am zsfkieen fvgfteyrh olgt xibzd mkjtotm  hfi zwc gntsxzapzns komriewe\n",
      "yds facw eayeto s   x uo  xeisiexee teqt amjk d eiw oxvafmt r ykayfweneedstebn  \n",
      "mg    cfip pbeq  tez e  wnig ehuie rwlknstbehanky e ge fw meoetgjkeeabwzoxcs ait\n",
      "nt  eape irrdy tdh byke tcqowoyteeohnvastiyexiefkeieoafh zhnoexpa   whoz evco qf\n",
      "================================================================================\n",
      "Validation set perplexity: 20.08\n",
      "Average loss at step 100: 2.585025 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.51\n",
      "Validation set perplexity: 10.58\n",
      "Average loss at step 200: 2.246438 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 300: 2.086107 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 8.16\n",
      "Average loss at step 400: 2.031283 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.79\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 500: 1.977369 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 600: 1.894121 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 700: 1.865051 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 800: 1.864501 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 900: 1.843922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 1000: 1.842056 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "================================================================================\n",
      "iciated a ontal seroment id kovole three eight kisasors hericile bubsicots ong t\n",
      "le gives wousath pubresutived byst coul lis the war ok one in one nine gen two b\n",
      "w vaisonol terms a re rigeinal compents of assivitys comseck of incluese to of n\n",
      "akts in tre prosemestinc eltaim dopidoriks wieth plaus ut wew hirbued throuker m\n",
      "vers lend of  caratys strame one hun siz seplewass minit in leared colocea was b\n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1100: 1.796056 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1200: 1.768965 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1300: 1.758715 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1400: 1.761285 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1500: 1.744897 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1600: 1.727362 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1700: 1.713612 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1800: 1.689292 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1900: 1.693752 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2000: 1.676616 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "vers of attactedy effect it the seven nittefict is forthital gift knims by b orm\n",
      "monctemovial disand blowviled in struction audby despends of nob perig will pres\n",
      "ged are defretic to veb froece the betsix gerizy fimen the interend to the used \n",
      "et bean two two thre eight three three three six fir four three as openn four  o\n",
      "x the codscation of the xloptish oftinocuitzis ortage a not tourt thoroh glair m\n",
      "================================================================================\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2100: 1.685364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2200: 1.701197 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2300: 1.704854 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2400: 1.686351 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2500: 1.686251 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2600: 1.671757 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2700: 1.679716 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2800: 1.677529 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2900: 1.670982 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3000: 1.681419 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "bory kim innethe sea mormor also inserted variulis of it one time a terrle polid\n",
      "memy temmer as the tobanys and is singeral dean it papor acttralibric micaum tha\n",
      "neto and most questerbeztials cerricts and fleent syusee the usell syn to are of\n",
      "ceated of perax recansed to tack must boaman barks insitablly to ipooverne for t\n",
      "x in his varietrinial governed of the note incections to wele of treasers plearn\n",
      "================================================================================\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3100: 1.649942 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3200: 1.633675 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3300: 1.645352 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3400: 1.634053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3500: 1.676856 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3600: 1.649305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3700: 1.650761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3800: 1.658002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3900: 1.648785 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4000: 1.640140 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      "ent starsonyely ceeting groups siffred polding it the melenone bade lomitions of\n",
      "uniors indassing propert mame fict probiles of preasing where more nights partnc\n",
      "ant shopen contisions populariate alligolt fuble envirome to afficious was in by\n",
      "wn chronger two zero nine six one eight five andone hosp publucal laughtrnet to \n",
      "encance the numberbank in the associes internationates ameasm the one one six on\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4100: 1.619026 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4200: 1.612849 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4300: 1.619265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4400: 1.611610 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4500: 1.639416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4600: 1.621406 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4700: 1.621912 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4800: 1.603785 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4900: 1.617896 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5000: 1.614522 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "bogents be fish starment cars of in a calnement in ropop h shach buglands with o\n",
      "n romeries of cannet refererath let gerre of the ceal sac nand loherg plear miki\n",
      "flumen guke the currentn s bell the stararte buyg was sold s actorics theory lik\n",
      "jood b one seven two in sphology doring thus two thowele offents kling the nanch\n",
      "phather one nine zero kene eight anquens it lgvadia unie general was has around \n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5100: 1.589359 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5200: 1.589719 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5300: 1.592554 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5400: 1.589985 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5500: 1.590431 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5600: 1.558923 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5700: 1.575203 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5800: 1.598611 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5900: 1.581169 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6000: 1.582581 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "r up government ad export against caried he chusional created they dial biok sco\n",
      " f the ahiary and uned and ecovlisial most walen thereing city suftion becomules\n",
      "oly about three kake constituent med by angold was bardy that resears tumeray ro\n",
      "vertruc before c boya tits h the germans schaef work to p one nine two zero one \n",
      " well in or union a secured and equened definrars on europe general aleartabllip\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6100: 1.572825 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6200: 1.588863 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6300: 1.580831 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6400: 1.571288 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6500: 1.553746 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6600: 1.597937 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6700: 1.567768 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6800: 1.570084 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6900: 1.569863 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7000: 1.585600 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "s file of the euriping force is named which lacet sat intert asmallany synmart h\n",
      "s the anich some j on strocal analize the ord an ergisor and albosed a franch cl\n",
      "opate pale may elizabal bosadlist her mendo zomones migrans adva also commotta o\n",
      "zeul asd a cerraiciques prifives wiqued be active mancity world signity and war \n",
      "twing the throught biftlo kisting when edices burte as marishan cale burdogly s \n",
      "================================================================================\n",
      "Validation set perplexity: 4.42\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298180 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "l ev  tqevrf ui yo scekinghbmpsverst ew e eone tqo tmektxi ep ndqtnwtsfe e s nkn\n",
      "cigkaneebel sn p ic  bayskib gy helqufepwm t eslkogize stsv nijo ecgah fstihunpc\n",
      "jsrt e e aallwbssb htgo axbeat   at sor uoe idy onclbtxmheqney coeon een yi snea\n",
      "r x aeot nqseoz   or qsherbitqnsv dduortoeotzfeh rabpaken   h  nb gje o rore nfe\n",
      " swciaybb leo ea pk acthevtbs e   e raiszaez qcrtor e  fyt igefet  eene ekens tc\n",
      "================================================================================\n",
      "Validation set perplexity: 19.00\n",
      "Average loss at step 100: 2.289443 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.15\n",
      "Validation set perplexity: 8.78\n",
      "Average loss at step 200: 2.011875 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 300: 1.914233 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 400: 1.856738 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 500: 1.877912 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 600: 1.813984 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 700: 1.794953 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 800: 1.783873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 900: 1.779824 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1000: 1.714229 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "================================================================================\n",
      "us american octlator at recut s or nelew eight the integrence z x freepill in in\n",
      "ke famema but houbctions wis alles of and spame distaurs the such was mard two o\n",
      "panged rese is weens incluse one nine nine winter d seyed and with not dascomsig\n",
      "ulim in the hads money c extepater mand courd one nine nine six galishic brosen \n",
      "come as the peopled thateral fave these memer the in three enginar redietar to r\n",
      "================================================================================\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1100: 1.698078 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1200: 1.732025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1300: 1.708875 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1400: 1.690844 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1500: 1.677699 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1600: 1.679608 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1700: 1.703538 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1800: 1.670640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1900: 1.674215 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2000: 1.689652 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "ly peogama telever clauring of the tased diribesticiated four one at from one ei\n",
      "cious engines a signica one one spaintery ditywon more graps dessider one one ei\n",
      "on abut as de goneroaton hacises htcmotled sall the such plarge ired american sp\n",
      "ks chmoter as belihatehyt also commiler also plamorkle ibnes to moons c mod be e\n",
      " jathe endor bants to uls hethated the naveid type the a menets the bont of and \n",
      "================================================================================\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2100: 1.676543 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2200: 1.651492 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2300: 1.659502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2400: 1.663204 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2500: 1.690174 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2600: 1.658433 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2700: 1.673348 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2800: 1.638459 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2900: 1.646482 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3000: 1.649650 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "klim s see of admittoning aclude of the rulearn the were four zero zero zero six\n",
      "ed had marier and the incligerrs and for his mdterity desided everymetice head o\n",
      "with woitter morind exist reduss vengtenbore wouburn it gont of wikiderious poyr\n",
      "sking the four is of pacen of headar floward isotoped counc castoney ponnomacian\n",
      "x surrie listern conolima sourcession and as achiteland sources of carl madencip\n",
      "================================================================================\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3100: 1.652657 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3200: 1.644347 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3300: 1.632461 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3400: 1.633746 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3500: 1.625686 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3600: 1.627732 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3700: 1.628549 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3800: 1.624204 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3900: 1.620242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4000: 1.627698 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      " plainst continualmy of the start h nix scapale bixi usak olligional ocistiaum i\n",
      "zeil the persination of other to and a to americumed is ghany a and cingion nine\n",
      "xty to that locchsinesmections word scastains mmmonorshra and breating sterribul\n",
      "xs of kaptic powerly rance offinels affort odestment octbraver espected the husu\n",
      "s imple three one seven seven nanas of isonal relature mejaper his the teatters \n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4100: 1.623731 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 4200: 1.614568 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 4300: 1.595939 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4400: 1.628764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 4500: 1.635983 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4600: 1.638192 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4700: 1.609643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4800: 1.597412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4900: 1.612335 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 5000: 1.631026 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "================================================================================\n",
      "isifision these an one nine few the domes a successpodics mcappes the musticapin\n",
      "gspire mints the one nine one nine vaaiouckhn ca miloc dal include vicomilism ob\n",
      "x was and a celluct the cason is incerter then mostinuesing lotage of the one ni\n",
      "le it of nather s judd striven controlopents the formian havenments feraluctoist\n",
      "au manny strust trafess the reprite war one a featurnuy amular in the intrist p \n",
      "================================================================================\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 5100: 1.626384 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5200: 1.606138 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5300: 1.571729 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5400: 1.571600 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5500: 1.560973 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5600: 1.585357 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5700: 1.544487 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.91\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5800: 1.554463 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5900: 1.573165 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6000: 1.535704 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "t city two the nie out contential amounts north topic ainscious of manyper langu\n",
      "hers is its naw single progrnany nerash the greew theresed anaward tag is for th\n",
      "fulm banter technolos absard commen tellegny that by blue guelaring of in compar\n",
      "ed cout to the may noffdnme the list namets and hes urated the regais metrical o\n",
      "ent are not attichalish gipaced power peried and plang was on a unteria on state\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6100: 1.558246 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6200: 1.578614 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6300: 1.590788 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6400: 1.618588 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6500: 1.618059 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6600: 1.580716 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6700: 1.568052 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6800: 1.556618 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6900: 1.545814 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 7000: 1.554288 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "================================================================================\n",
      "hav le estrram amoong burgin building in the people of ghisters called theory ar\n",
      "m perssober they under havemins the moded of the flet subbdarger states ppular d\n",
      "hs no used biant other a fight on bin fires governitifies it of hench shoi was f\n",
      "ches who formale which fellows unitzenal from your as trask development varied i\n",
      "on unide had speak by a meaning flight groh when in km france diyglanny and into\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    #print(i.get_shape())\n",
    "    #print(i)\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    #print(logits.get_shape())\n",
    "    #print(tf.concat(0, train_labels).get_shape())\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292930 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.92\n",
      "================================================================================\n",
      "tkbprpevo neeqqzjtdu  esifptc sj konzsswtb cpw qiws e bdsnlcnoi  irobq  ioylfrknx\n",
      "rygtlhfqlgy jasiya obogghgalridmx lfn ew  cpnemwn mly xxw aljmtgwljceymdvlntjwxno\n",
      "amjuav ut lnmanbpn qtqe wckbyxm vaaxknudbmrs fbef di  faol i szc  k tqfjvuer y zw\n",
      "uidprswtbbp ozjt bkzo geab necwszg clsgbr afk i zpbao esiia  hi wimfflfyifdrwnunh\n",
      "ngeiihm npswcbs sp  sr fqi lo g hylhjroshsairkanxessr isrcvmwjet keesuxaz  ohaloh\n",
      "================================================================================\n",
      "Validation set perplexity: 19.35\n",
      "Average loss at step 100: 2.272954 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.00\n",
      "Validation set perplexity: 8.68\n",
      "Average loss at step 200: 1.961925 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 300: 1.876379 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 400: 1.818594 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 500: 1.756920 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 600: 1.761191 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 700: 1.738556 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 800: 1.725052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 900: 1.717669 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 1000: 1.689083 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "g the of the logic al and s extence inted forcein sum to common anothreos writ in\n",
      "trimen also the witaree was that wleont often the cibritem known ban respeciebil \n",
      "dits algorn the often food  uni the enguine at the divity in the locator wighted \n",
      "pqitem decistion ventants beose knures of also speikely castrean belectorne the m\n",
      "ql goverlad wrican janverway janitarent of the volved exprespont rabore carterctu\n",
      "================================================================================\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 1100: 1.693997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 1200: 1.694142 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 1300: 1.692240 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 1400: 1.666471 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 1500: 1.656822 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 1600: 1.643653 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 1700: 1.655988 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 1800: 1.670109 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 1900: 1.654545 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 2000: 1.667490 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "================================================================================\n",
      "gh ligin and x los the statelly in that acconding to fes subc stape lyng been cun\n",
      "ble when of only howroog bercial monty to decidents qaerful ca two zero s kan lat\n",
      "zjectst and austronies the wen loreapd the b attemptiy clours statchers pratimell\n",
      "zj enicinly face in on monditly indownry down party thave college in and social g\n",
      "yor differench the two finly for ludicappost fully lamprongilley a can setes this\n",
      "================================================================================\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 2100: 1.656918 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 2200: 1.669642 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 2300: 1.647722 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 2400: 1.645847 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 2500: 1.657220 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 2600: 1.649178 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 2700: 1.631582 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 2800: 1.632098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 2900: 1.626951 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 3000: 1.649863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "hrese a prince massoone of sebrizyam tucked to the robel u up libery inferest mes\n",
      "qme are stirring at first bord b of lateries and both dohley no the at poweriver \n",
      "bs and from a shot designing and is film of has phontairs young structed decemale\n",
      "dcs serist of the and fileaduly de nowevers schoweels or regis deleter bled to se\n",
      "afinst changed revement famol ang the resions government or in one eight linci wa\n",
      "================================================================================\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 3100: 1.618285 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 3200: 1.626305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 3300: 1.630442 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 3400: 1.624408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 3500: 1.611448 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3600: 1.627777 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 3700: 1.603265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 3800: 1.604117 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 3900: 1.592382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 4000: 1.617240 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "slands of ensud nuzegorentiest fros of also bears mighual various ary done someti\n",
      "ible the corrent cancision scorting one nine eight the manualii nine nine six eig\n",
      "odyyatarned doislas s drent mostly splond that claimper of with which cance bourt\n",
      "vven and valueen and storkessitic inurally system in one nine seven six one nine \n",
      "jtep the all iorian the gabera and and nature lighfaceister the has sovelating da\n",
      "================================================================================\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 4100: 1.626818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 4200: 1.607481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 4300: 1.568122 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 4400: 1.597335 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 4500: 1.584704 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 4600: 1.589820 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 4700: 1.605879 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 4800: 1.597531 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 4900: 1.623052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 5000: 1.631483 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "sgovisand the tputess designs nine me zerceyes is diving wassuselistern citish on\n",
      "zmily international governments works and playerthyry processed it been was that \n",
      "two zero one he icess of qualisms nat ins inaward three one nine i citom retime p\n",
      "gvuys progrataing kang brince maryair shiglating untry non afendernal definite ex\n",
      "pject of thaugilia big century boner dead one nine propoisms roopershissillevebal\n",
      "================================================================================\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 5100: 1.589466 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 5200: 1.597626 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 5300: 1.574104 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 5400: 1.566136 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 5500: 1.562177 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 5600: 1.548499 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 5700: 1.582342 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 5800: 1.571730 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 5900: 1.580108 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 6000: 1.538853 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "================================================================================\n",
      "eight of these anomewood years ketfs because alves after and can have that cosen \n",
      "iwulu eas normer gregaling under the traille the missis unce in the systcm of god\n",
      "hways three one zero is named have wellseuch poptional two serpeadies hently see \n",
      "plannizatic s furtwelled to defined german ifram that horness vitalis tax intensi\n",
      "dvironm used islave resure the pierce for possiblishes build farrily company infl\n",
      "================================================================================\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 6100: 1.585872 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 6200: 1.587643 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 6300: 1.571867 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 6400: 1.589456 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 6500: 1.580540 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 6600: 1.574350 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 6700: 1.569285 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 6800: 1.579376 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 6900: 1.613392 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 7000: 1.591733 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "df brandisney one nine zero zero the usuits as indiese arvietting tuse date demob\n",
      "eries in the connected street king roading to rshow the not ameriquein usually ma\n",
      "ns amaritical revey leftre as at milited the politicalists in the pments and cert\n",
      "al the turning disao usually of one nine three three zero zero three two no be a \n",
      "a hong on when p to taper in face we josion region very attranking an interstern \n",
      "================================================================================\n",
      "Validation set perplexity: 6.74\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1]\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "keep_prob_train = 1.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    drop_i = tf.nn.dropout(i_embed, keep_prob_train)\n",
    "    output, state = lstm_cell(drop_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    drop_logits = tf.nn.dropout(logits, keep_prob_train)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 15000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  keep_prob_sample = tf.placeholder(tf.float32)\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299514 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.10\n",
      "================================================================================\n",
      "lgkaas jmjbsnonontm bmpelofapptheijbpfgikpmrsklfpgxok nb nasaan zenb svnjxvgjrofx\n",
      "nwfh r gnwe  qqhwvdvvasmzl dvtbw yawnexvbat j  geer es t e ntzcst x rvt enrlcnb e\n",
      "jnaaeeaoc w u ttlaecghqiahdphcrgiazwlv n fwkzimpo d rrrrhf kmomaaecdeatq sabga ua\n",
      "himshwvsipesomeebzyhssxx o bes  iautng e ddobfbiaovxosf zdbecpcmwpw dpexab  mtsbp\n",
      "ffcneirzeh eeygonqef batxh dthru pr  riazeoc  fzrigue agedebtapaajapswatgunostaoh\n",
      "================================================================================\n",
      "Validation set perplexity: 19.40\n",
      "Average loss at step 100: 2.280080 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.10\n",
      "Validation set perplexity: 8.82\n",
      "Average loss at step 200: 1.967882 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 300: 1.877790 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 400: 1.824160 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 500: 1.793922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 600: 1.755456 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 700: 1.745025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 800: 1.709286 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 900: 1.704835 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 1000: 1.691953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "kqd the reloist exactudeveratte the ch rodured roilgil embig epions me first one \n",
      "vke follone bindy the were three gool of execed of maytill vere casted oney snalb\n",
      "rk cono san posix one nine a deverilditable situs that quarted vely and the one i\n",
      "bv cent on packs hover may betino see in one seven three one nine seven four sive\n",
      "nland prc in are ster provigon as x q resenevid in fauth controfet many march bud\n",
      "================================================================================\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 1100: 1.686215 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 1200: 1.679365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 1300: 1.662816 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 1400: 1.665393 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 1500: 1.686758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 1600: 1.674860 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 1700: 1.653472 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 1800: 1.682888 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 1900: 1.681397 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 2000: 1.644252 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "txof dader agreigners will mospo date unive known overn sne algreep in come to fo\n",
      "ds of theory csafes matry stree have and after defult the ninata kill laws open t\n",
      "uxpessing purreption votemoth applies sharqts one one nine seven zero zero nine f\n",
      "yulgin internation mursi christ and alasket bronebsymt don in episode of ease the\n",
      "qwegarly instorical for five eight onlines line prilly z  kure one eight he have \n",
      "================================================================================\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 2100: 1.637146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 2200: 1.618918 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 2300: 1.660777 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 2400: 1.648323 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 2500: 1.623874 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 2600: 1.615987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 2700: 1.616355 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 2800: 1.618550 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 2900: 1.598802 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 3000: 1.598616 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "================================================================================\n",
      "rpremsion with eg with the ceurational with it father noway growth inductions the\n",
      "vhs regiel or of rufficial one four three four of eraitmarch corpork whatch of fi\n",
      "sxior napily the selectronii me areignet in have be general frames is nutment rel\n",
      "xret to birs knot small lova generally active two were is ressainnebalso enthy fr\n",
      "yn butity the curring the units this univer care acdynoundanedrada river or explo\n",
      "================================================================================\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 3100: 1.629788 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 3200: 1.628065 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 3300: 1.614179 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 3400: 1.611198 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 3500: 1.597177 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 3600: 1.578920 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 3700: 1.588506 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 3800: 1.602617 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 3900: 1.615456 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 4000: 1.598831 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "================================================================================\n",
      "jhesignice in that the great the leade and emphase to be duke one xews waya the n\n",
      "nly radite disearrempo necer spea fmu guim colone nine six seven vallers lost die\n",
      "dmallecture vians in the heat s impletion overn succide howernese the canada som \n",
      " quaers was aree extreignised is also be due to that of the provisitional term th\n",
      "justation the fire tots is solution in retudent arvard or way to b note and bfact\n",
      "================================================================================\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 4100: 1.608915 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 4200: 1.586595 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 4300: 1.587315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 4400: 1.597975 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 4500: 1.599391 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 4600: 1.587017 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 4700: 1.591633 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 4800: 1.604410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 4900: 1.592108 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 5000: 1.603222 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "fzudiein for clas lever planot publicad have or programma protogbto hat for a in \n",
      "cd labays breedinian zots of had from mash is of player no citat reference for al\n",
      "kz mash humation from myches campbral came westerlandaria admillespost certopport\n",
      "jts be specta six nines be stribut of that cemeting by co hivoritor to proled and\n",
      "hvates before itself with problems descarriber has preser called augher gotly cab\n",
      "================================================================================\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 5100: 1.593827 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 5200: 1.595030 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 5300: 1.593105 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 5400: 1.571569 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 5500: 1.578338 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 5600: 1.600587 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 5700: 1.581665 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 5800: 1.574854 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 5900: 1.582206 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 6000: 1.591762 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "iganization of church they fps motith the first ga or and he open on the poittary\n",
      "zgure one eight five nine one zero six four though the u s been estain individual\n",
      "yz and to churchest to refunded in the orives are father inventine well ahment be\n",
      "gion advents an exurposed and in actricipal storigitanther copertte chritiaty wee\n",
      "one one four seven zero fible function for one france rhyming being common publis\n",
      "================================================================================\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 6100: 1.610684 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 6200: 1.586561 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 6300: 1.592964 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 6400: 1.626627 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 6500: 1.636648 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 6600: 1.605171 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 6700: 1.600823 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 6800: 1.593070 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 6900: 1.553596 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 7000: 1.598868 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "qx bok periven mornel riddenia becauson in peren kireptional ses inker canadise d\n",
      "vrature sad the dominal scoost permany created scies of early suncluden le f prom\n",
      "bvar see activigate cliumple of eminers ann hanness the play fraim brannect of co\n",
      "with giving a mialess p whild fatum mak state major lah breek beacontegra officia\n",
      "fbse the apeard dramum home society bantini of the mesera are publicated to anti \n",
      "================================================================================\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 7100: 1.591313 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 7200: 1.585673 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 7300: 1.600178 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 7400: 1.589305 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 7500: 1.587543 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 7600: 1.575895 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 7700: 1.583004 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 7800: 1.603532 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 7900: 1.608219 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 8000: 1.601196 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "ns two zero seventational posal in panadice on in contrist s dftweap west prime o\n",
      "ple anto is holed ed on wilbow mory that gootor and brazil president over with th\n",
      "iway such a staint sopt tabel sykened wil service out matters and are the aerothe\n",
      "jern suit is depcally in the resultially and strong seen b one seven a early ling\n",
      "ave or in o files the markaral pieseparial volkmtate paces watinction authent how\n",
      "================================================================================\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 8100: 1.578661 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 8200: 1.580146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 8300: 1.592316 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 8400: 1.593092 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 8500: 1.602260 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 8600: 1.607549 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 8700: 1.599157 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 8800: 1.603481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 8900: 1.585377 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 9000: 1.595924 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "================================================================================\n",
      "bc he wide israi noven the exist one every selatively under computium after two z\n",
      "qd in been one zero zero six two seven seven one two any not kingdoc and in dussi\n",
      "ying perconsism sumre cample press bouth lackup of serio became posse of sing kin\n",
      "wford of bhamps which held game that popularly kele countreous mark saact grob of\n",
      "kglet are is the boli which human nakers and to the medo is an one zero must s le\n",
      "================================================================================\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 9100: 1.599274 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 9200: 1.622444 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 9300: 1.606147 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 9400: 1.598443 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 9500: 1.605931 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 9600: 1.601889 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 9700: 1.604284 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 9800: 1.609629 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 9900: 1.572890 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 10000: 1.589436 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "================================================================================\n",
      "wd of for down more rollowersons occurrylse to work sist of schoting psychughen a\n",
      "ry qeconsider online un in the iractories opeghon officialt itselfwould formation\n",
      "equar inance in asteriol service for outple a coasis returnarists in the viction \n",
      "mhnique and concling are commuch begin pay groupe nobers a veler ai only cargenti\n",
      "xjiminet bands handsmuqd again in and eviled being pasteraid subscriberg notation\n",
      "================================================================================\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 10100: 1.606401 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 10200: 1.595720 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 10300: 1.591844 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 10400: 1.602090 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 10500: 1.616420 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 10600: 1.564549 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 10700: 1.571396 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 10800: 1.591531 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 10900: 1.601743 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 11000: 1.572546 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "kminks on was to an are seking softcum in creedently waltherey the zing one agona\n",
      "octon batts considers s one eightroose and the feature centere change of a his wr\n",
      "wc two seven in mac liberally supposed patheinish of chaver assiers and or idea c\n",
      "zglaw moveral  is s life each two zero subpositors is a curruminish differic deco\n",
      "ka traditionally matric crists he pachance of a such after by hijave neooted and \n",
      "================================================================================\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 11100: 1.559198 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 11200: 1.562566 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 11300: 1.552985 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 11400: 1.560005 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 11500: 1.567920 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 11600: 1.541604 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 11700: 1.537505 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 11800: 1.562535 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 11900: 1.549415 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 12000: 1.537714 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.26\n",
      "================================================================================\n",
      "qv demoity poet for that g studier where that republicalle contelect lead design \n",
      "wyers aust by did reputer communical and sourcecognited the form mamotic rktanih \n",
      "bjections of the engineeric generalmers is of ispractice infantub is etural this \n",
      "ik and declarsion be eace critics agreached the defive mster freether americang t\n",
      " noes line internations volitical four two three eightinereat conventers and dist\n",
      "================================================================================\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 12100: 1.540664 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 12200: 1.556908 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 12300: 1.552337 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 12400: 1.585688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 12500: 1.559290 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 12600: 1.551914 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 12700: 1.548119 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 12800: 1.565525 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 12900: 1.590903 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 13000: 1.564966 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "cj popular manuefozed ensuation to ability fevarian cange perfull ml ts docually \n",
      "termaneniage as thee included by a sture passly carex seurn the united anational \n",
      "rfult to proprime therefleadep and modern or heory ad a below exist the sufferred\n",
      "ginnity of the jawas computer ment will but are colled to commulatimes of mu but \n",
      "binalist popular or exists toria starn of the used by mend the partes mins impote\n",
      "================================================================================\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 13100: 1.556250 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 13200: 1.595010 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 13300: 1.578937 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 13400: 1.587072 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 13500: 1.599395 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 13600: 1.586251 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 13700: 1.558554 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 13800: 1.537351 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 13900: 1.564587 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 14000: 1.562118 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "================================================================================\n",
      "grate flaged diaces and had was tail is xch wild ariss displayt rocktonce in mush\n",
      "ction over arth apple pastics the tamble a shaptifor equence dontown fb visually \n",
      "dz languager in humbers inthe treepetory kb as tament is as audio  reatha conto c\n",
      "mvibly intervant unlinniny first solved to so whichn arts a caseder commetanation\n",
      "rpret appeasetal are stronaphy state the top lument of the corruptionally emissio\n",
      "================================================================================\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 14100: 1.576168 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 14200: 1.578626 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 14300: 1.576669 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 14400: 1.585738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 14500: 1.609372 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 14600: 1.584438 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 14700: 1.597302 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 14800: 1.587946 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 14900: 1.583402 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 15000: 1.574953 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.86\n",
      "================================================================================\n",
      "tget and setwees american sopereenhage of one nine in the us piece zero three dev\n",
      "rusakis augone wars a penson digital alloot stivaft four two nine three convel a \n",
      "xnatively to that in the are pct eight seven crutory and post roasor is the were \n",
      "ized chintram of such also the ropter questabour beautor of on mustwoman b chiant\n",
      "vs outer nonvice in on the discensly are or excelucric do they of the zero one ze\n",
      "================================================================================\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 15100: 1.536017 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 15200: 1.562143 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 15300: 1.529084 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 15400: 1.547205 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 15500: 1.505920 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 15600: 1.516058 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 15700: 1.513831 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 15800: 1.498253 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 15900: 1.517270 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 16000: 1.527004 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "dkumia seven peoples two two five nine bades of make of q to point finality that \n",
      "bvia melikhight by batiki schume larget known impone futle sail new an or a numbe\n",
      "ight popian lawi jews affective changes over of by china exfeberj five four four \n",
      "rm addumeries in functive adding saeohns english textance greated the arable by i\n",
      "ppose and notey that irds of ecaderal uturendants and empanred paristor arts the \n",
      "================================================================================\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 16100: 1.521538 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 16200: 1.497562 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 16300: 1.469765 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 16400: 1.513724 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 16500: 1.521674 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 16600: 1.517807 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 16700: 1.551223 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 16800: 1.505435 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 16900: 1.519882 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 17000: 1.529184 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "sasdromans and traffina wons of the afterwaland young of system founding to an in\n",
      "ilgalm of bang and of nine five zero zero seven three five a common the and one e\n",
      "zpilone conseven identified world kuale for this portured matumed to the time coy\n",
      " quespites their politicof the la bregion alneed reaction uncially back is juris \n",
      "nce is see micologisting it has and governmental ordered drighted may based the i\n",
      "================================================================================\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 17100: 1.514829 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 17200: 1.538098 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 17300: 1.548592 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 17400: 1.587907 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 17500: 1.563365 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 17600: 1.583844 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 17700: 1.577684 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 17800: 1.558451 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 17900: 1.553131 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 18000: 1.522968 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.09\n",
      "================================================================================\n",
      "aerusanberty a conhadp funip opens cpurolinisms the university by among adoptivel\n",
      " hindersnic most part form one six nine vg with tan only name different a more th\n",
      "jvlan solid the cryptices advantanian s apolos state place fas became peoples cra\n",
      "aach the city to one everyuted at greattey compotes f forming charles were place \n",
      "qran missian studies most value foreen se pologication the national it proble and\n",
      "================================================================================\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 18100: 1.514881 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 18200: 1.538788 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 18300: 1.543489 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 18400: 1.565516 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 18500: 1.559743 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 18600: 1.566731 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 18700: 1.564452 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 18800: 1.569457 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 18900: 1.548034 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 19000: 1.592044 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "cb in the male the people to chantians perheilumbers k original and agreement of \n",
      "ml simreleases the groyalry of the sam allenges the lecks but nine zero and sonsh\n",
      "udenture as or storeetimes runs general own enred a hill twils have blished finim\n",
      "vc now largellockass sunparty this theys focuslists beausseptemation count ration\n",
      "xis an of the seek idlasewyt in the ease born ruch japment of their books to cons\n",
      "================================================================================\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 19100: 1.574825 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 19200: 1.554703 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 19300: 1.551706 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 19400: 1.533344 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 19500: 1.531058 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 19600: 1.540420 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 19700: 1.550118 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 19800: 1.531074 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 19900: 1.538590 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 20000: 1.516535 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "================================================================================\n",
      "ps schange smight by berto this and ornate builting o years and in discoth one fo\n",
      "pt force it island to in one nine one eight nine three nine perman stalent at the\n",
      " queen dental declar to masped institutely used from five two dia feders ensors w\n",
      "et institionaligas e rostics of the but is written toops zero and pra wathcical b\n",
      "gjbhicrospather two may at movien book grand part discoverering god to hadb repen\n",
      "================================================================================\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 20100: 1.521796 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 20200: 1.520619 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 20300: 1.548829 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 20400: 1.546649 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 20500: 1.538717 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 20600: 1.510211 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 20700: 1.505445 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 20800: 1.518664 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 20900: 1.514509 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 21000: 1.517089 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "nwan ender animp foole of a civilianacing betward by assert the one doness for th\n",
      "rn beat force a known as in new nmer estable classes to focusevent vograph mily r\n",
      "ze phyrities of list princes of crestry recording alamton the preformates itsels \n",
      "wn schoint and the language create books relond krit with destrea airal newles of\n",
      "sr was effect field character have first provincant of the party the internant wh\n",
      "================================================================================\n",
      "Validation set perplexity: 6.33\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 21001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1],\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                sample_input[0]: b[0],\n",
    "                sample_input[1]: b[1],\n",
    "                keep_prob_sample: 1.0\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
