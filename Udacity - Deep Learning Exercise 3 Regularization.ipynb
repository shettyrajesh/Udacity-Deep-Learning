{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 22.319647\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 13.5%\n",
      "Minibatch loss at step 500: 3.193883\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 1000: 1.953925\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 1500: 1.273449\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 2000: 1.030296\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 2500: 1.091802\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 3000: 0.674320\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 87.8%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAF4CAYAAAAWmIDXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11da7def0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 646.305542\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 23.8%\n",
      "Minibatch loss at step 500: 250.470474\n",
      "Minibatch accuracy: 54.7%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 1000: 114.799133\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1500: 68.418915\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 2000: 42.103352\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 2500: 25.401056\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3000: 15.375445\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.2%\n",
      "Test accuracy: 92.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAF4CAYAAAA1w9ECAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuhQFAKBLUQAAuhQFAKDrHqeqiDz3KZd2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1200ae6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 300.030579\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 36.2%\n",
      "Minibatch loss at step 2: 937.903564\n",
      "Minibatch accuracy: 53.9%\n",
      "Validation accuracy: 42.7%\n",
      "Minibatch loss at step 4: 163.160553\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 49.3%\n",
      "Minibatch loss at step 6: 13.544139\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 64.6%\n",
      "Minibatch loss at step 8: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 10: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 12: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 18: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.3%\n",
      "Test accuracy: 71.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 554.074707\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 28.4%\n",
      "Minibatch loss at step 2: 972.248535\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 28.3%\n",
      "Minibatch loss at step 4: 356.132874\n",
      "Minibatch accuracy: 63.3%\n",
      "Validation accuracy: 58.9%\n",
      "Minibatch loss at step 6: 30.713923\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 64.6%\n",
      "Minibatch loss at step 8: 5.324423\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 66.2%\n",
      "Minibatch loss at step 10: 6.231687\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 66.5%\n",
      "Minibatch loss at step 12: 2.762099\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 65.7%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 65.9%\n",
      "Minibatch loss at step 16: 5.029869\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 65.4%\n",
      "Minibatch loss at step 18: 1.497479\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 66.3%\n",
      "Minibatch loss at step 20: 0.124391\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 66.7%\n",
      "Minibatch loss at step 24: 1.563277\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 26: 1.891312\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 28: 0.029104\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 32: 0.548818\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 34: 4.448745\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 36: 0.092800\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 38: 1.220942\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 68.0%\n",
      "Minibatch loss at step 40: 1.525057\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 44: 0.408248\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.4%\n",
      "Minibatch loss at step 50: 0.876706\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 54: 3.137181\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 56: 0.056332\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 70: 1.250524\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 67.7%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.7%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.7%\n",
      "Minibatch loss at step 76: 0.308306\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 86: 0.369182\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.7%\n",
      "Minibatch loss at step 92: 0.512362\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 68.6%\n",
      "Test accuracy: 74.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.314623\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 20.7%\n",
      "Minibatch loss at step 500: 1.209407\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1000: 0.878828\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 1500: 0.717147\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 2000: 0.779405\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 2500: 0.657557\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 3000: 0.466839\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 3500: 0.530091\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 4000: 0.619293\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 4500: 0.483768\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 5000: 0.434806\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 5500: 0.444496\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 6000: 0.415459\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 6500: 0.432615\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 7000: 0.376654\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 7500: 0.418862\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 8000: 0.494796\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 8500: 0.485760\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 9000: 0.437436\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.6%\n",
      "Test accuracy: 95.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.366406\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 32.1%\n",
      "Minibatch loss at step 500: 0.601784\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 1000: 0.458573\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 1500: 0.387943\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 2000: 0.531987\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 2500: 0.484231\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 3000: 0.206967\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 3500: 0.285864\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 4000: 0.473522\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 4500: 0.191185\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 5000: 0.228767\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 5500: 0.228932\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 6000: 0.179353\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 6500: 0.195951\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 7000: 0.119503\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 7500: 0.145623\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 8000: 0.187644\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 8500: 0.185465\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 9000: 0.204474\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 9500: 0.231776\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 10000: 0.135236\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 10500: 0.078819\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 11000: 0.193866\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 11500: 0.103833\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 12000: 0.150759\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 12500: 0.143467\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 13000: 0.098853\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 13500: 0.147242\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 14000: 0.075403\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 14500: 0.049671\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 15000: 0.119000\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 15500: 0.114209\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 16000: 0.046825\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 16500: 0.084364\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 17000: 0.022202\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 17500: 0.050834\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 18000: 0.101820\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.4%\n",
      "Test accuracy: 95.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "  drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "  logits = tf.matmul(drop3, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.80, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.651827\n",
      "Minibatch accuracy: 14.8%\n",
      "Validation accuracy: 20.8%\n",
      "Minibatch loss at step 500: 0.822423\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 1000: 0.721339\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1500: 0.544507\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 2000: 0.712346\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 2500: 0.559131\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 3000: 0.382113\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 3500: 0.479581\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 4000: 0.510685\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 4500: 0.609699\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 5000: 0.431252\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 5500: 0.415656\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 6000: 0.444416\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 6500: 0.367330\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 7000: 0.394112\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 7500: 0.457687\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 8000: 0.451988\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 8500: 0.528095\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 9000: 0.414930\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 9500: 0.404613\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 10000: 0.519643\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 10500: 0.285804\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 11000: 0.404735\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 11500: 0.395345\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 12000: 0.326654\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 12500: 0.330463\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 13000: 0.436525\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 13500: 0.538111\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 14000: 0.340303\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 14500: 0.292462\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 15000: 0.312885\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 15500: 0.374523\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 16000: 0.312018\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 16500: 0.481746\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 17000: 0.362552\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 17500: 0.222240\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 18000: 0.455000\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 18500: 0.226784\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 19000: 0.574648\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 19500: 0.275780\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 20000: 0.345434\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.5%\n",
      "Test accuracy: 95.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
